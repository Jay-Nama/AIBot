# -*- coding: utf-8 -*-
"""Handwriting_Detection_(C).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1HMnCKKhyECa3r8wGoXkgdi009vNiYDjd

# **Data Collection (IAM)**
"""

# Download and extract IAM Handwriting dataset
!wget -q https://git.io/J0fjL -O IAM_Words.zip

!unzip -qq IAM_Words.zip

!mkdir data
!mkdir data/words
!tar -xf IAM_Words/words.tgz -C data/words
!mv IAM_Words/words.txt data

# Display the database information and format
!head -20 data/words.txt

!pip install tensorflow numpy pandas matplotlib opencv-python pillow scipy h5py seaborn scikit-learn

from google.colab import drive
drive.mount('/content/drive')

from tensorflow.keras.layers import StringLookup
from tensorflow import keras
import matplotlib.pyplot as plt
import tensorflow as tf

import numpy as np
import os
from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score

np.random.seed(42)
tf.random.set_seed(42)

"""# **Dataset Splitting**"""

# Extract labels from words.txt
base_path = "/content/data"
words_list = []

words = open(f"{base_path}/words.txt", "r").readlines()
for line in words:
  if line[0] =="#": # Don't extract the header information
    continue
  if line.split(" ")[1] != "err":
    words_list.append(line)

len(words_list)

np.random.shuffle(words_list)

print(words_list[0:10])

# Split Data: 75% train, 12.5% validation, 12.5% test
split_idx = int(0.75*len(words_list))
train_samples = words_list[:split_idx]
test_samples = words_list[split_idx:]

val_split_idx = int(0.5 * len(test_samples))
validation_samples = test_samples[:val_split_idx]
test_samples = test_samples[val_split_idx:]

assert len(words_list) == len(train_samples) + len(validation_samples) + len(test_samples)

print(f"Total Training Samples: {len(train_samples)}")
print(f"Total validation samples: {len(validation_samples)}")
print(f"Total test samples: {len(test_samples)}")

"""# **Data Pipeline**"""

# Add image path names and label annotation to data pipeline
base_image_path = os.path.join(base_path, "words")
print(base_path)
def get_image_paths_and_labels(samples):
  paths = []
  corrected_samples = []
  # Store each path in this format: base_image_path/partI/partI-partII/image_name.png
  for (i, file_line) in enumerate(samples):
    line_split = file_line.strip()
    line_split = line_split.split(" ")

    image_name = line_split[0]
    partI = image_name.split("-")[0]
    partII = image_name.split("-")[1]
    img_path = os.path.join(
        base_image_path, partI, partI + "-" + partII, image_name + ".png"
    )
    if os.path.getsize(img_path):
      paths.append(img_path)
      corrected_samples.append(file_line.split("\n")[0])

  return paths, corrected_samples

train_img_paths, train_labels = get_image_paths_and_labels(train_samples)
validation_img_paths, validation_labels = get_image_paths_and_labels(validation_samples)
test_img_paths, test_labels = get_image_paths_and_labels(test_samples)

train_img_paths[0:10]

train_labels_cleaned = []
characters = set() # Create an empty set to store each individual character
max_len = 0

# Extract label name from annotation
for label in train_labels:
  label = label.split(" ")[-1].strip()
  for char in label:
    characters.add(char)

  max_len = max(max_len, len(label)) # Finding size of the longest word
  train_labels_cleaned.append(label)

print("Maximum length: ", max_len)
print("Vocab size: ", len(characters))

# Check some label samples
train_labels_cleaned[:10]

# Extract validation and test labels
def clean_labels(labels):
  cleaned_labels = []
  for label in labels:
    label = label.split(" ")[-1].strip()
    cleaned_labels.append(label)
  return cleaned_labels

validation_labels_cleaned = clean_labels(validation_labels)
test_labels_cleaned = clean_labels(test_labels)

"""**Building the Character Vocabulary**

To make sure the order of the character list is consistent, we use the pickle library to store it. We can load the charcater list whenever we want to make sure the characters are in the correct order.
"""

# ff = list(characters)

#import pickle
#with open("/content/drive/MyDrive/Colab Notebooks/characters", "wb") as fp:   #Pickling (store character list)
#    pickle.dump(ff, fp)

import pickle
with open("/content/drive/MyDrive/Colab Notebooks/characters", "rb") as fp:   # Unpickling (get character list)
    b = pickle.load(fp)
    print(b)

#Build Character Vocabulary
AUTOTUNE = tf.data.AUTOTUNE

# Maping characaters to integers
char_to_num = StringLookup(vocabulary=b, mask_token=None)

# Maping integers back to original characters using an index
num_to_chars = StringLookup(vocabulary=char_to_num.get_vocabulary(), mask_token=None, invert=True)

# Resize images without distorting (add padding to make all images the same size)
def distortion_free_resize(image, img_size):
  w, h = img_size
  image = tf.image.resize(image, size=(h, w), preserve_aspect_ratio=True)

  # Check the amount of padding needed to be done
  pad_height = h - tf.shape(image)[0]
  pad_width = w - tf.shape(image)[1]

  # Add padding on both sides
  if pad_height % 2 != 0:
    height = pad_height // 2
    pad_height_top = height + 1
    pad_height_bottom = height
  else:
    pad_height_top = pad_height_bottom = pad_height // 2

  if pad_width % 2 != 0:
    width = pad_width // 2
    pad_width_left = width + 1
    pad_width_right = width
  else:
    pad_width_left = pad_width_right = pad_width // 2

  image = tf.pad(
      image, paddings=[
          [pad_height_top, pad_height_bottom],
          [pad_width_left, pad_width_right],
          [0, 0],
      ],
  )
  image = tf.transpose(image, perm=[1,0,2])
  image = tf.image.flip_left_right(image)
  return image

# Functions to preprocess images and labels into a TensorFlow acceptable format
batch_size = 64
padding_token = 99
image_width = 128
image_height = 32

# Preprocess Images (resize and normalize)
def preprocess_image(image_path, img_size=(image_width, image_height)):
  image = tf.io.read_file(image_path)
  image = tf.image.decode_png(image, 1)
  image = distortion_free_resize(image, img_size)
  image = tf.cast(image, tf.float32) / 255.0
  return image

# Map each character to an integer (index) in the character set
def vectorize_label(label):
  label = char_to_num(tf.strings.unicode_split(label, input_encoding="UTF-8"))
  length = tf.shape(label)[0]
  pad_amount = max_len - length # Add padding so that all labels are the same length
  label = tf.pad(label, paddings=[[0, pad_amount]], constant_values=padding_token)
  return label

# Combine image and label
def process_images_labels(image_path, label):
  image = preprocess_image(image_path)
  label = vectorize_label(label)
  return {"image": image, "label": label}

# Create TensorFlow data pipeline
def prepare_dataset(image_paths, labels):
  dataset = tf.data.Dataset.from_tensor_slices((image_paths, labels)).map(
    process_images_labels, num_parallel_calls=AUTOTUNE
  )
  return dataset.batch(batch_size).cache().prefetch(AUTOTUNE)

# Insert images and label name into data pipeline
train_ds = prepare_dataset(train_img_paths, train_labels_cleaned)
validation_ds = prepare_dataset(validation_img_paths, validation_labels_cleaned)
test_ds = prepare_dataset(test_img_paths, test_labels_cleaned)

"""**Displaying Test Samples**"""

# Display some samples
for data in train_ds.take(1):
  images, labels = data["image"], data["label"]
  _, ax = plt.subplots(4, 4, figsize=(15, 8))

  # 16 samples
  for i in range(16):
    img = images[i]
    img = tf.image.flip_left_right(img)
    img = tf.transpose(img, perm=[1, 0, 2])
    img = (img * 255.0).numpy().clip(0, 255).astype(np.uint8)
    img = img[:, :, 0]

    # Convert index to string label
    label = labels[i]
    indices = tf.gather(label, tf.where(tf.math.not_equal(label, padding_token)))
    label = tf.strings.reduce_join(num_to_chars(indices))
    label = label.numpy().decode("utf-8")

    ax[i // 4, i % 4].imshow(img, cmap="gray")
    ax[i // 4, i % 4].set_title(label)
    ax[i // 4, i % 4].axis("off")

  plt.show() # Padding shown for each sample in black

"""# **Building Model**"""

# CTC Class to calculate CTC loss during training
class CTCLayer(keras.layers.Layer):

  def __init__(self, name=None, **kwargs):
    super().__init__(name=name, **kwargs)
    self.loss_fn = keras.backend.ctc_batch_cost

  # Return the predictions (y_pred) from the model
  def call(self, y_true, y_pred):
    batch_len = tf.cast(tf.shape(y_true)[0], dtype="int64")
    input_length = tf.cast(tf.shape(y_pred)[1], dtype="int64")
    label_length = tf.cast(tf.shape(y_true)[1], dtype="int64")

    input_length = input_length * tf.ones(shape=(batch_len, 1), dtype="int64")
    label_length = label_length * tf.ones(shape=(batch_len, 1), dtype="int64")
    loss = self.loss_fn(y_true, y_pred, input_length, label_length) # Calculate CTC loss
    self.add_loss(loss)

    return y_pred

# Construct handwriting recognition model
def build_model():
  input_img = keras.Input(shape=(image_width, image_height, 1), name="image") # Input layer takes in grayscale image
  labels = keras.layers.Input(name="label", shape=(None,))

  # CNN to extract features from images (edges and lines)
  # 2D Convolutional Layer
  x = keras.layers.Conv2D(
      32, (3,3), activation = "relu",
      kernel_initializer="he_normal",
      padding="same",
      name="Conv1")(input_img)
  # 2D Max Pooling Layer (Reduce dimensions, but still retain imporant info)
  x = keras.layers.MaxPooling2D((2,2), name="pool1")(x)

  # Second 2D Convolutional Layer
  x = keras.layers.Conv2D(
      64, (3,3), activation = "relu", kernel_initializer="he_normal",
      padding="same",
      name="Conv2")(x)
  x = keras.layers.MaxPooling2D((2,2), name="pool2")(x) # Second Max Pooling Layer

  # Reshape layers to transition from CNN to RNN layers
  new_shape = ((image_width // 4), (image_height // 4) * 64)
  x = keras.layers.Reshape(target_shape=new_shape, name="reshape")(x)
  x = keras.layers.Dense(64, activation="relu", name="dense1")(x) # Dense layer to connect CNN and RNN
  x = keras.layers.Dropout(0.2)(x)

  # RNN (Recurrent Neural Network) for sequence data (character by character)
  # Bidirectional LSTM layers (recognize dependencies and patterns)
  x = keras.layers.Bidirectional(
      keras.layers.LSTM(128, return_sequences=True, dropout=0.25))(x)
  x = keras.layers.Bidirectional(
    keras.layers.LSTM(64, return_sequences=True, dropout=0.25))(x)

  # Output dense layer with the length of the character set to predict the class
  # +2 is to account for any padding that may have been added
  x = keras.layers.Dense(
    len(char_to_num.get_vocabulary()) + 2, activation="softmax", name="dense2")(x)

  # CTC layer for calculating CTC Loss at each step.
  output = CTCLayer(name="ctc_loss")(labels, x)

  # Define model and optimizer
  model = keras.models.Model(
      inputs=[input_img, labels], outputs=output, name="handwriting_recognizer"
  )
  opt = keras.optimizers.Adam()

  # Compile the model and return
  model.compile(optimizer=opt)
  return model

# Get the model
model = build_model()

model.summary()

"""# **Callback Function**"""

# Split validation set into images and labels to use in callback function
validation_images = []
validation_labels = []

for batch in validation_ds:
  validation_images.append(batch["image"])
  validation_labels.append(batch["label"])

# Edit Distance is the most widely used metric for evaluating OCR models.
# Measure similarity between gorund truth labels and model predictions
def calculate_edit_distance(labels, predictions):
  # Get a single batch and convert its labels to sparse tensors.
  sparse_labels = tf.cast(tf.sparse.from_dense(labels), dtype=tf.int64)

  # Decode predictions
  input_len = np.ones(predictions.shape[0]) * predictions.shape[1]
  predictions_decoded = keras.ops.nn.ctc_decode(predictions, sequence_lengths=input_len)[0][0][:, :max_len]
  sparse_predictions = tf.cast(
    tf.sparse.from_dense(predictions_decoded), dtype=tf.int64
  )

  # Compute individual edit distances and average them out.
  edit_distances = tf.edit_distance(
    sparse_predictions, sparse_labels, normalize=False
  )
  return tf.reduce_mean(edit_distances)

# Class to track the distance after each epoch
class EditDistanceCallback(keras.callbacks.Callback):
  def __init__(self, pred_model):
    super().__init__()
    self.prediction_model = pred_model
  def on_epoch_end(self, epoch, logs = None):
    edit_distances = []

    # Iteratre over validation images and find distance
    for i in range(len(validation_images)):
      labels = validation_labels[i]
      predictions = self.prediction_model.predict(validation_images[i])
      edit_distances.append(calculate_edit_distance(labels, predictions).numpy())
    print(f"Mean edit distance for each {epoch + 1}: {np.mean(edit_distances): .4f}")

"""# **Train Model**"""

epochs = 50

model = build_model()
prediction_model = keras.models.Model(
  model.get_layer(name="image").output, model.get_layer(name="dense2").output
)
edit_distance_callback = EditDistanceCallback(prediction_model)

# Train the model
history = model.fit(
  train_ds,
  validation_data=validation_ds,
  epochs=epochs,
  callbacks=[edit_distance_callback],
)

# Save model
model.save('/content/drive/MyDrive/Colab Notebooks/50_model.h5')
prediction_model.save('/content/drive/MyDrive/Colab Notebooks/50_prediction_model.h5')

# import cv2
# import os
# from tensorflow.keras.models import load_model

# Load model
# model_path = "/content/drive/MyDrive/Colab Notebooks/50_model.h5"
# prediction_model_path = "/content/drive/MyDrive/Colab Notebooks/50_prediction_model.h5"
# model = load_model(model_path, custom_objects = {"CTCLayer": CTCLayer})
# prediction_model = load_model(prediction_model_path)

"""# **Test Model**"""

import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix

# Decode predictions into readable text
def decode_batch_predictions(pred):
    input_len = np.ones(pred.shape[0]) * pred.shape[1]
    # Use greedy search to select the highest confidence character
    results = keras.backend.ctc_decode(pred, input_length=input_len, greedy=True)[0][0][:, :max_len]

    # Iterate over the results and get back the text.
    output_text = []

    # Convert character set indexes into character using mapping
    for res in results:
      res = tf.gather(res, tf.where(tf.math.not_equal(res, -1)))
      res = tf.strings.reduce_join(num_to_chars(res)).numpy().decode("utf-8")
      output_text.append(res)

    return output_text

# Check predictions from some test samples.
for batch in test_ds.take(1):
    batch_images = batch["image"]
    print(batch_images.shape)

    _, ax = plt.subplots(4, 4, figsize=(15, 8))

    preds = prediction_model.predict(batch_images)
    pred_texts = decode_batch_predictions(preds)

    for i in range(16):
      img = batch_images[i]
      img = tf.image.flip_left_right(img)
      img = tf.transpose(img, perm=[1, 0, 2])
      img = (img * 255.0).numpy().clip(0, 255).astype(np.uint8)
      img = img[:, :, 0]

      title = f"Prediction: {pred_texts[i]}"
      ax[i // 4, i % 4].imshow(img, cmap = "gray")
      ax[i // 4, i % 4].set_title(title)
      ax[i // 4, i % 4].axis("off")

    plt.show()

# here we are evaluating the metrics to judge the model word accuracy, f-1 score, recall, precision
def metricsEvaluation(y_true, y_pred):

    # calculkate the accuracy, F1, recall, and precision
    calcAcc = accuracy_score(y_true, y_pred)
    calcf1 = f1_score(y_true, y_pred, average="weighted", zero_division=1)
    calcRecall = recall_score(y_true, y_pred, average="weighted", zero_division=1)
    calcPrecision = precision_score(y_true, y_pred, average="weighted", zero_division=1)

    # here we are output the metrics
    print(f"Word Accuracy: {calcAcc:.2f}")
    print(f"Word F1 Score: {calcf1:.2f}")
    print(f"Word Recall: {calcRecall:.2f}")
    print(f"Word Precision: {calcPrecision:.2f}")
    return calcAcc, calcf1, calcRecall, calcPrecision

# here is the function to evaluate the model on the test dataset
def modEVal(test_ds, prediction_model, char_to_num):
    # store labels
    groundTruthLabels = []
    predictedLabels = []

    # here we are iterating through the test dataset batches
    for batch in test_ds:
        batch_images = batch["image"]
        batch_labels = batch["label"]

        # predict text for the images
        preds = prediction_model.predict(batch_images)
        pred_texts = decode_batch_predictions(preds)

        # here we are converting ground truth lable to readable text
        for label in batch_labels:
            indices = tf.gather(label, tf.where(tf.math.not_equal(label, padding_token)))
            Text = tf.strings.reduce_join(num_to_chars(indices)).numpy().decode("utf-8")
            groundTruthLabels.append(Text)

        # here we are appending decodede predictions
        predictedLabels.extend(pred_texts)

    # here we are calculating and displaying the metrics
    metricsEvaluation(groundTruthLabels, predictedLabels)

# Call the function
modEVal(test_ds, prediction_model, char_to_num)

"""# **Input Custom Image**"""

def preprocess_image_from_array(image_array, img_size=(128, 32)):

    # Convert NumPy array to TensorFlow tensor
    image = tf.convert_to_tensor(image_array, dtype=tf.uint8)

    # Ensure the image is grayscale
    if len(image.shape) == 3:  # RGB
        image = tf.image.rgb_to_grayscale(image)

    # Resize without distortion
    image = distortion_free_resize(image, img_size)

    # Normalize pixel values to [0, 1]
    image = tf.cast(image, tf.float32) / 255.0
    return image

def segment_words_and_predict(img, prediction_model, preprocess_func, img_size=(128, 32)):
    # Preprocess input image (grayscale + thresholding)
    def thresholding(image):
        img_gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
        _, thresh = cv2.threshold(img_gray, 200, 255, cv2.THRESH_BINARY_INV)
        return thresh

    thresh_img = thresholding(img)

    # Line segmentation
    kernel = np.ones((3, 85), np.uint8)
    dilated = cv2.dilate(thresh_img, kernel, iterations=1)
    contours, _ = cv2.findContours(dilated.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)
    sorted_contours_lines = sorted(contours, key=lambda ctr: cv2.boundingRect(ctr)[1])  # Sort by Y-coordinate

    # Text segmentation
    kernel = np.ones((3, 15), np.uint8)
    dilated_words = cv2.dilate(thresh_img, kernel, iterations=1)
    words_list = []

    # Make a copy of the image to draw rectangles
    segmented_image = img.copy()

    for line in sorted_contours_lines:
        # Get bounding box coordinates
        x, y, w, h = cv2.boundingRect(line)
        roi_line = dilated_words[y:y+h, x:x+w]

        # Extract words from each line
        cnt, _ = cv2.findContours(roi_line.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)
        sorted_contour_words = sorted(cnt, key=lambda cntr: cv2.boundingRect(cntr)[0])  # Sort by X-coordinate

        for word in sorted_contour_words:
            if cv2.contourArea(word) < 400:
                continue  # Skip small noise contours

            x2, y2, w2, h2 = cv2.boundingRect(word)
            word_box = [x+x2, y+y2, x+x2+w2, y+y2+h2]
            words_list.append(word_box)

            # Draw a rectangle around each word
            cv2.rectangle(segmented_image, (word_box[0], word_box[1]), (word_box[2], word_box[3]), (255, 0, 0), 2)

    # Preprocess each word crop and predict text by feeding it into the prediction model
    preprocessed_words = []
    for word_box in words_list:
        x1, y1, x2, y2 = word_box
        word_crop = img[y1:y2, x1:x2]

        # Convert word crop to TensorFlow tensor and preprocess (resize and add padding)
        word_crop_tensor = tf.convert_to_tensor(word_crop, dtype=tf.uint8)
        preprocessed_word = preprocess_func(word_crop_tensor, img_size=img_size)
        preprocessed_words.append(preprocessed_word)

    # Create a batch for model prediction
    preprocessed_words = tf.stack(preprocessed_words)

    # Predict using the model
    predictions = prediction_model.predict(preprocessed_words)

    # Decode predictions
    decoded_texts = decode_batch_predictions(predictions)

    # Combine each word into a final message
    final_output = " ".join(decoded_texts)

    return final_output, segmented_image

import cv2
# Add custom image
img = cv2.imread('/content/drive/MyDrive/Colab Notebooks/demo.png')
img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)

final_text, segmented_img = segment_words_and_predict(img, prediction_model, preprocess_image_from_array, img_size=(128, 32))

# Show segmented image
plt.imshow(segmented_img)
plt.show()

# Print final message
print("Final Predicted Text:")
print(final_text)